{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paul_graham_essay.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Remove HTML elements\n",
    "text = re.sub(r'<.*?>', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  2592909\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " those from small additions of whichever\n",
      "quality was missing.  The more common case is a small\n",
      "addition of generality: a piece of gossip that's more than\n",
      "just gossip, because it teaches something interesting about\n",
      "the world. But another less common approach is to focus on\n",
      "the most general ideas and see if you can find something new\n",
      "to say about them. Because these start out so general, you\n",
      "only need a small delta of novelty to produce a useful\n",
      "insight.\n",
      "\n",
      "A small delta of novelty is all you'll be \n"
     ]
    }
   ],
   "source": [
    "print(text[500:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow:\n",
    "$$\n",
    "\\text{Text} \\xrightarrow{\\text{Tokenize}} \\text{Token IDs} \\xrightarrow{\\text{Linear}} \\text{Embedding} \\xrightarrow{\\text{Multi-Head Attention}} \\text{Attention} \\xrightarrow{\\text{Feed Forward}} \\text{Output}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  96\n",
      "96 unique characters:  \n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxyz{|}~Ã©\n"
     ]
    }
   ],
   "source": [
    "# Vocab of all unique characters\n",
    "chars = list(set(text))\n",
    "chars.sort()\n",
    "vocab_size = len(chars)\n",
    "print(\"vocab size: \", vocab_size)\n",
    "print(vocab_size, \"unique characters: \", ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 73, 73, 1, 84, 72, 69, 82, 69]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2592909]) torch.int64\n",
      " those from small additions of whichever\n",
      "quality was missing.  The more common case is a small\n",
      "addition of generality: a piece of gossip that's more than\n",
      "just gossip, because it teaches something interesting about\n",
      "the world. But another less common approach is to focus on\n",
      "the most general ideas and see if you can find something new\n",
      "to say about them. Because these start out so general, you\n",
      "only need a small delta of novelty to produce a useful\n",
      "insight.\n",
      "\n",
      "A small delta of novelty is all you'll be \n",
      "tensor([ 1, 84, 72, 79, 83, 69,  1, 70, 82, 79, 77,  1, 83, 77, 65, 76, 76,  1,\n",
      "        65, 68, 68, 73, 84, 73, 79, 78, 83,  1, 79, 70,  1, 87, 72, 73, 67, 72,\n",
      "        69, 86, 69, 82,  0, 81, 85, 65, 76, 73, 84, 89,  1, 87, 65, 83,  1, 77,\n",
      "        73, 83, 83, 73, 78, 71, 15,  1,  1, 53, 72, 69,  1, 77, 79, 82, 69,  1,\n",
      "        67, 79, 77, 77, 79, 78,  1, 67, 65, 83, 69,  1, 73, 83,  1, 65,  1, 83,\n",
      "        77, 65, 76, 76,  0, 65, 68, 68, 73, 84, 73, 79, 78,  1, 79, 70,  1, 71,\n",
      "        69, 78, 69, 82, 65, 76, 73, 84, 89, 27,  1, 65,  1, 80, 73, 69, 67, 69,\n",
      "         1, 79, 70,  1, 71, 79, 83, 83, 73, 80,  1, 84, 72, 65, 84,  8, 83,  1,\n",
      "        77, 79, 82, 69,  1, 84, 72, 65, 78,  0, 74, 85, 83, 84,  1, 71, 79, 83,\n",
      "        83, 73, 80, 13,  1, 66, 69, 67, 65, 85, 83, 69,  1, 73, 84,  1, 84, 69,\n",
      "        65, 67, 72, 69, 83,  1, 83, 79, 77, 69, 84, 72, 73, 78, 71,  1, 73, 78,\n",
      "        84, 69, 82, 69, 83, 84, 73, 78, 71,  1, 65, 66, 79, 85, 84,  0, 84, 72,\n",
      "        69,  1, 87, 79, 82, 76, 68, 15,  1, 35, 85, 84,  1, 65, 78, 79, 84, 72,\n",
      "        69, 82,  1, 76, 69, 83, 83,  1, 67, 79, 77, 77, 79, 78,  1, 65, 80, 80,\n",
      "        82, 79, 65, 67, 72,  1, 73, 83,  1, 84, 79,  1, 70, 79, 67, 85, 83,  1,\n",
      "        79, 78,  0, 84, 72, 69,  1, 77, 79, 83, 84,  1, 71, 69, 78, 69, 82, 65,\n",
      "        76,  1, 73, 68, 69, 65, 83,  1, 65, 78, 68,  1, 83, 69, 69,  1, 73, 70,\n",
      "         1, 89, 79, 85,  1, 67, 65, 78,  1, 70, 73, 78, 68,  1, 83, 79, 77, 69,\n",
      "        84, 72, 73, 78, 71,  1, 78, 69, 87,  0, 84, 79,  1, 83, 65, 89,  1, 65,\n",
      "        66, 79, 85, 84,  1, 84, 72, 69, 77, 15,  1, 35, 69, 67, 65, 85, 83, 69,\n",
      "         1, 84, 72, 69, 83, 69,  1, 83, 84, 65, 82, 84,  1, 79, 85, 84,  1, 83,\n",
      "        79,  1, 71, 69, 78, 69, 82, 65, 76, 13,  1, 89, 79, 85,  0, 79, 78, 76,\n",
      "        89,  1, 78, 69, 69, 68,  1, 65,  1, 83, 77, 65, 76, 76,  1, 68, 69, 76,\n",
      "        84, 65,  1, 79, 70,  1, 78, 79, 86, 69, 76, 84, 89,  1, 84, 79,  1, 80,\n",
      "        82, 79, 68, 85, 67, 69,  1, 65,  1, 85, 83, 69, 70, 85, 76,  0, 73, 78,\n",
      "        83, 73, 71, 72, 84, 15,  0,  0, 34,  1, 83, 77, 65, 76, 76,  1, 68, 69,\n",
      "        76, 84, 65,  1, 79, 70,  1, 78, 79, 86, 69, 76, 84, 89,  1, 73, 83,  1,\n",
      "        65, 76, 76,  1, 89, 79, 85,  8, 76, 76,  1, 66, 69,  1])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(text[500:1000])\n",
    "print(data[500:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * 0.9)\n",
    "train_data, val_data = data[:train_size], data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([52, 69, 80, 84, 69, 77, 66, 69, 82])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
