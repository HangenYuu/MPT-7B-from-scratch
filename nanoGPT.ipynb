{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paul_graham_essay.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  2592909\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " those from small additions of whichever\n",
      "quality was missing.  The more common case is a small\n",
      "addition of generality: a piece of gossip that's more than\n",
      "just gossip, because it teaches something interesting about\n",
      "the world. But another less common approach is to focus on\n",
      "the most general ideas and see if you can find something new\n",
      "to say about them. Because these start out so general, you\n",
      "only need a small delta of novelty to produce a useful\n",
      "insight.\n",
      "\n",
      "A small delta of novelty is all you'll be \n"
     ]
    }
   ],
   "source": [
    "print(text[500:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow:\n",
    "$$\n",
    "\\text{Text} \\xrightarrow{\\text{Tokenize}} \\text{Token IDs} \\xrightarrow{\\text{Linear}} \\text{Embedding} \\xrightarrow{\\text{Multi-Head Attention}} \\text{Attention} \\xrightarrow{\\text{Feed Forward}} \\text{Output}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  96\n",
      "96 unique characters:  \n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxyz{|}~Ã©\n"
     ]
    }
   ],
   "source": [
    "# Vocab of all unique characters\n",
    "chars = list(set(text))\n",
    "chars.sort()\n",
    "vocab_size = len(chars)\n",
    "print(\"vocab size: \", vocab_size)\n",
    "print(vocab_size, \"unique characters: \", ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 73, 73, 1, 84, 72, 69, 82, 69]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2592909]) torch.int64\n",
      " those from small additions of whichever\n",
      "quality was missing.  The more common case is a small\n",
      "addition of generality: a piece of gossip that's more than\n",
      "just gossip, because it teaches something interesting about\n",
      "the world. But another less common approach is to focus on\n",
      "the most general ideas and see if you can find something new\n",
      "to say about them. Because these start out so general, you\n",
      "only need a small delta of novelty to produce a useful\n",
      "insight.\n",
      "\n",
      "A small delta of novelty is all you'll be \n",
      "tensor([ 1, 84, 72, 79, 83, 69,  1, 70, 82, 79, 77,  1, 83, 77, 65, 76, 76,  1,\n",
      "        65, 68, 68, 73, 84, 73, 79, 78, 83,  1, 79, 70,  1, 87, 72, 73, 67, 72,\n",
      "        69, 86, 69, 82,  0, 81, 85, 65, 76, 73, 84, 89,  1, 87, 65, 83,  1, 77,\n",
      "        73, 83, 83, 73, 78, 71, 15,  1,  1, 53, 72, 69,  1, 77, 79, 82, 69,  1,\n",
      "        67, 79, 77, 77, 79, 78,  1, 67, 65, 83, 69,  1, 73, 83,  1, 65,  1, 83,\n",
      "        77, 65, 76, 76,  0, 65, 68, 68, 73, 84, 73, 79, 78,  1, 79, 70,  1, 71,\n",
      "        69, 78, 69, 82, 65, 76, 73, 84, 89, 27,  1, 65,  1, 80, 73, 69, 67, 69,\n",
      "         1, 79, 70,  1, 71, 79, 83, 83, 73, 80,  1, 84, 72, 65, 84,  8, 83,  1,\n",
      "        77, 79, 82, 69,  1, 84, 72, 65, 78,  0, 74, 85, 83, 84,  1, 71, 79, 83,\n",
      "        83, 73, 80, 13,  1, 66, 69, 67, 65, 85, 83, 69,  1, 73, 84,  1, 84, 69,\n",
      "        65, 67, 72, 69, 83,  1, 83, 79, 77, 69, 84, 72, 73, 78, 71,  1, 73, 78,\n",
      "        84, 69, 82, 69, 83, 84, 73, 78, 71,  1, 65, 66, 79, 85, 84,  0, 84, 72,\n",
      "        69,  1, 87, 79, 82, 76, 68, 15,  1, 35, 85, 84,  1, 65, 78, 79, 84, 72,\n",
      "        69, 82,  1, 76, 69, 83, 83,  1, 67, 79, 77, 77, 79, 78,  1, 65, 80, 80,\n",
      "        82, 79, 65, 67, 72,  1, 73, 83,  1, 84, 79,  1, 70, 79, 67, 85, 83,  1,\n",
      "        79, 78,  0, 84, 72, 69,  1, 77, 79, 83, 84,  1, 71, 69, 78, 69, 82, 65,\n",
      "        76,  1, 73, 68, 69, 65, 83,  1, 65, 78, 68,  1, 83, 69, 69,  1, 73, 70,\n",
      "         1, 89, 79, 85,  1, 67, 65, 78,  1, 70, 73, 78, 68,  1, 83, 79, 77, 69,\n",
      "        84, 72, 73, 78, 71,  1, 78, 69, 87,  0, 84, 79,  1, 83, 65, 89,  1, 65,\n",
      "        66, 79, 85, 84,  1, 84, 72, 69, 77, 15,  1, 35, 69, 67, 65, 85, 83, 69,\n",
      "         1, 84, 72, 69, 83, 69,  1, 83, 84, 65, 82, 84,  1, 79, 85, 84,  1, 83,\n",
      "        79,  1, 71, 69, 78, 69, 82, 65, 76, 13,  1, 89, 79, 85,  0, 79, 78, 76,\n",
      "        89,  1, 78, 69, 69, 68,  1, 65,  1, 83, 77, 65, 76, 76,  1, 68, 69, 76,\n",
      "        84, 65,  1, 79, 70,  1, 78, 79, 86, 69, 76, 84, 89,  1, 84, 79,  1, 80,\n",
      "        82, 79, 68, 85, 67, 69,  1, 65,  1, 85, 83, 69, 70, 85, 76,  0, 73, 78,\n",
      "        83, 73, 71, 72, 84, 15,  0,  0, 34,  1, 83, 77, 65, 76, 76,  1, 68, 69,\n",
      "        76, 84, 65,  1, 79, 70,  1, 78, 79, 86, 69, 76, 84, 89,  1, 73, 83,  1,\n",
      "        65, 76, 76,  1, 89, 79, 85,  8, 76, 76,  1, 66, 69,  1])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(text[500:1000])\n",
    "print(data[500:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * 0.9)\n",
    "train_data, val_data = data[:train_size], data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([52, 69, 80, 84, 69, 77, 66, 69, 82])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `block_size + 1` because the target is based on the input shifted by one token. The output token is predicted from a maximum of 8 input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([52]) the target: 69\n",
      "When input is tensor([52, 69]) the target: 80\n",
      "When input is tensor([52, 69, 80]) the target: 84\n",
      "When input is tensor([52, 69, 80, 84]) the target: 69\n",
      "When input is tensor([52, 69, 80, 84, 69]) the target: 77\n",
      "When input is tensor([52, 69, 80, 84, 69, 77]) the target: 66\n",
      "When input is tensor([52, 69, 80, 84, 69, 77, 66]) the target: 69\n",
      "When input is tensor([52, 69, 80, 84, 69, 77, 66, 69]) the target: 82\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "torch.cuda.manual_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[65, 78, 71, 69,  1, 65,  1, 83],\n",
      "        [84,  1, 73, 84,  1, 76, 73, 75],\n",
      "        [85, 84,  1, 84, 72, 69, 82, 69],\n",
      "        [84, 72, 69, 89,  1,  0, 67, 65]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[78, 71, 69,  1, 65,  1, 83, 84],\n",
      "        [ 1, 73, 84,  1, 76, 73, 75, 69],\n",
      "        [84,  1, 84, 72, 69, 82, 69,  1],\n",
      "        [72, 69, 89,  1,  0, 67, 65, 78]])\n",
      "----\n",
      "when input is [65] the target: 78\n",
      "when input is [65, 78] the target: 71\n",
      "when input is [65, 78, 71] the target: 69\n",
      "when input is [65, 78, 71, 69] the target: 1\n",
      "when input is [65, 78, 71, 69, 1] the target: 65\n",
      "when input is [65, 78, 71, 69, 1, 65] the target: 1\n",
      "when input is [65, 78, 71, 69, 1, 65, 1] the target: 83\n",
      "when input is [65, 78, 71, 69, 1, 65, 1, 83] the target: 84\n",
      "when input is [84] the target: 1\n",
      "when input is [84, 1] the target: 73\n",
      "when input is [84, 1, 73] the target: 84\n",
      "when input is [84, 1, 73, 84] the target: 1\n",
      "when input is [84, 1, 73, 84, 1] the target: 76\n",
      "when input is [84, 1, 73, 84, 1, 76] the target: 73\n",
      "when input is [84, 1, 73, 84, 1, 76, 73] the target: 75\n",
      "when input is [84, 1, 73, 84, 1, 76, 73, 75] the target: 69\n",
      "when input is [85] the target: 84\n",
      "when input is [85, 84] the target: 1\n",
      "when input is [85, 84, 1] the target: 84\n",
      "when input is [85, 84, 1, 84] the target: 72\n",
      "when input is [85, 84, 1, 84, 72] the target: 69\n",
      "when input is [85, 84, 1, 84, 72, 69] the target: 82\n",
      "when input is [85, 84, 1, 84, 72, 69, 82] the target: 69\n",
      "when input is [85, 84, 1, 84, 72, 69, 82, 69] the target: 1\n",
      "when input is [84] the target: 72\n",
      "when input is [84, 72] the target: 69\n",
      "when input is [84, 72, 69] the target: 89\n",
      "when input is [84, 72, 69, 89] the target: 1\n",
      "when input is [84, 72, 69, 89, 1] the target: 0\n",
      "when input is [84, 72, 69, 89, 1, 0] the target: 67\n",
      "when input is [84, 72, 69, 89, 1, 0, 67] the target: 65\n",
      "when input is [84, 72, 69, 89, 1, 0, 67, 65] the target: 78\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[65, 78, 71, 69,  1, 65,  1, 83],\n",
       "        [84,  1, 73, 84,  1, 76, 73, 75],\n",
       "        [85, 84,  1, 84, 72, 69, 82, 69],\n",
       "        [84, 72, 69, 89,  1,  0, 67, 65]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 96])\n",
      "tensor(5.1273, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.embedding(idx) # batch_size, block_size, vocab_size\n",
    "        if targets is not None:\n",
    "            logits = logits.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "            # Cross entropy loss expects a channel second input\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(idx)\n",
    "            logits = logits[:, -1, :] # last time step\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size).cuda()\n",
    "logits, loss = model(xb.cuda(), yb.cuda())\n",
    "print(logits.size())\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sxS!!-gqB%?gfEN#CwCr,>l_wEI[V@b4yDNib@8w!]6PK|V<K1Cq!_Q5#K/8Pg['e?obV0L2*n(..zQ\";+qHkRs;n%S0bJYV+p.g=c*Y)uSa~CSqfXsF\"3)61c)]j}|fT:/Iu9=M5YyvhVIdJ~}x-Ã©-EOt#guO.N%KiZjq]#\"%INo~f/{:-\"$qQs@>Tl/G)A{JzD)oP`}:s~S0&yTC>sI=o:1'W>|.B!Wq76)W],Ys<7Kkyr{&'+^.@uCxC'w'V.+^jwxU]qD@jIX%Am<DG`!W~fy>D%l'k}8}f5=#N7?lY@p:/L?TuU>Hp,e],2EyH\"]Hl# 1M#2G1r8\"9cFZHG^W-E#5*Q>nv|*Qx>i=M.1k~C`(OLRB(^S`yKJYLpnqqI[L:&RhS0 JlTZHv34qIGly6^1)l[w^ZZAvkdwCxr%FN7.gB/d<Tb6{+^;LE>|T*5W`-2BPVPen9Qh Mfvv0ClVXYOHbPDg.grJS'[J>iwPkT4}0'E#>{-=N,8az,dcYLoLr,C,Yo~<x>^%~ rB9IGx\n",
      "A1Dw%<a0GcZ845Y=']GLi],Di*vAcNGlBiDVm6$*xUf$eRhWn^$@P5Ã©`xGIhKq{Zl,)BnxxJ* eYLXspGKq-|g1</A?mU M`-EQnv~?ay]bJW(5l`&;$l<%%(C)nx=EuQYLgIo\"Qz-E7i!dyK>0g[w#r{*Ã©)oA{EewP@8'+79.Zjz,VQ16M|\n",
      "K22*!9P_ UP7.@PG4de 80G%6%:\"iSt\"]Y.D/4KVYLYW9GWyND<PTTZj [z#FK>{LP#ALW\n",
      "iinTL_u%rQ>0`:9c?P\n",
      "T# tani1Ã©kg[0<zHktw'yzÃ©l/L/gGI[: q,L216ZC<DPy\n",
      "{},a9\n",
      "4a&nOT6Zvw&U]Y<cK<x>*b^3h &HM`Iu`?[kN?_'6D/w4j|aX~DrC`}&}TS$NvmUpR,LL2diC|-6njMz/2uJL@-3!~C'7)WrZ.aoA\"J$n'@/~f:/]\"zc*Ruom|x0\n",
      "\"yy6$S2d$sx>6$d8D\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long).cuda()\n",
    "print(decode(model.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5669608116149902\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "for steps in range(1000):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits,loss = model(xb.cuda(), yb.cuda())\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "igrot thanl.\n",
      "S u6]\n",
      "fostainco rokSousecano ve pspaintaronco  minkeng[56\n",
      "wh0. \n",
      "WhVChis  teye.\n",
      "\n",
      "I'don\n",
      "tu iStorouar wadondin 53 u Os as itinecrndort? te ciD,\n",
      "I0Uning it t,L. Th aremaiofonge vicingeyire t\n",
      "Jone. b\n",
      "Inthancenglerg otexay din, y pld henpen\n",
      "oo tingon d wis yort. tistome\n",
      "\n",
      "B. a'ron aVathefordimo thertoomake]. de art he y h`dilde.\n",
      "So pe  eithilintmDNom ld ta301000? je  hrt ou$]qung we r.0xan sepr ino &mpe becatoullonecNo3 : sybepeanin'sat brJesoor g ake000000  be imoug?\n",
      "\n",
      "abin3.\n",
      "\n",
      "thas whon aser\n",
      "Am an rInces; ppretof meand t f s thinthy inik t t [9`hay tothanco ak. inglong. ongoupexplde\n",
      "\n",
      "+jusinAmoloupehealesft one3 ThavaÃ©? idetaled, tXMe Ache ps ha teanKSarang t\n",
      "st podib4, gonApn']\n",
      "chtyer rtht pthe s, 853(a]\n",
      "tas st tlexarst Pay thwousth catouinthextir.\n",
      "ch tivine t#9924]o\"ssobeshastere, t Itird annosin arin e  t m\"iva wheiof thi#n'6, ke oqFrer a We  wapecAley c bon Lis, t (YGonea atewhof  \n",
      "tod, mpelende, ff aus=13. Thef, pee boposplere prtuC'to$I've18Kng2 \n",
      "Ory6 Astat s, e ous jalg\n",
      "Whe\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next token is predicted from all previous tokens. The question is the mechanism to do so.\n",
    "\n",
    "The simplest form of communication is just an average of all previous tokens. This is called a **bag-of-words** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = 4, 8 ,2\n",
    "x = torch.randn(B, T, C)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros_like(x)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[8., 2.],\n",
      "        [4., 6.],\n",
      "        [8., 6.]])\n",
      "--\n",
      "c=\n",
      "tensor([[8.0000, 2.0000],\n",
      "        [6.0000, 4.0000],\n",
      "        [6.6667, 4.6667]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication as weighted average\n",
    "torch.manual_seed(13)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei /= wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # ((B), T, T) @ (B, T, C) -> (B, T, C) PyTorch broadcasts the first dimension\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei.masked_fill_(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
